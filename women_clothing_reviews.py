# -*- coding: utf-8 -*-
"""women-clothing-reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rKDu45fKioppqOhGVQzqd02WnYccv58D

# <center> Text Classification with womens-clothing-reviews data set</center>

I am very glad to welcome you all to my notebook, In this notebook I'm going to work on a text classification problem. The problem is described as **'Given Review about clothing on E-commerce predict whether the custoomer will recommed it to her friends or not'...**<br/><br/>
I need to mention one thing I'm new to Machine Learning with text and I have used very easy and effective approaches to solve this problem. I have done some data analysis, data visualizations then finally build both machine learning and deep learning models.
It's time to jump on the process but before that I will mention the Workflow:<br/><br/>
**1)Loading the data<br/>
2)Handling Missing Values<br/>
3)Cleaning the data<br/>
4)Data Analysis and Visualization<br/>
5)Handling MultiColinearity<br/>
6)Tokenisation+stemming+corpus creation<br/>
7)Buidling ML model using Bag of words<br/>
8)Building ML model using Tf-Idf Vectoriztion<br/>
9)Deep Learning Model with Embeddings<br/>
10)Checking the model with new example**
"""

import numpy as np
import pandas as pd
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# Loading the Data"""

data = pd.read_csv('/kaggle/input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv',index_col =[0])

data.head(2)

data.shape

"""# Checking For Missing Values and Handling it"""

data.isnull().sum()/len(data)*100

data.info()

data.drop(labels =['Clothing ID','Title'],axis = 1,inplace = True) #Dropping unwanted columns

data[data['Review Text'].isnull()]

data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review

data.shape

"""# Data Analysis and Visualization"""

import plotly.express as px

px.histogram(data, x = 'Age')

px.histogram(data, x = data['Rating'])

px.histogram(data, x = data['Class Name'])

px.scatter(data, x="Age", y="Positive Feedback Count", facet_row="Recommended IND", facet_col="Rating",trendline="ols",category_orders={"Rating": [1,2,3,4,5],'Recommended IND':[0,1]})

px.violin(data, x="Age", y="Department Name", orientation="h", color="Recommended IND")

px.box(data, x="Age", y="Division Name", orientation="h",color = 'Recommended IND')

"""# Cleaning the Text Data"""

err1 = data['Review Text'].str.extractall("(&amp)")
err2 = data['Review Text'].str.extractall("(\xa0)")

print('with &amp',len(err1[~err1.isna()]))
print('with (\xa0)',len(err2[~err2.isna()]))

data['Review Text'] = data['Review Text'].str.replace('(&amp)','')
data['Review Text'] = data['Review Text'].str.replace('(\xa0)','')

err1 = data['Review Text'].str.extractall("(&amp)")
print('with &amp',len(err1[~err1.isna()]))
err2 = data['Review Text'].str.extractall("(\xa0)")
print('with (\xa0)',len(err2[~err2.isna()]))

!pip install TextBlob
from textblob import *

data['polarity'] = data['Review Text'].map(lambda text: TextBlob(text).sentiment.polarity)

data['polarity']

px.histogram(data, x = 'polarity')

px.box(data, y="polarity", x="Department Name", orientation="v",color = 'Recommended IND')

data['review_len'] = data['Review Text'].astype(str).apply(len)

px.histogram(data, x = 'review_len')

data['token_count'] = data['Review Text'].apply(lambda x: len(str(x).split()))

px.histogram(data, x = 'token_count')

"""# Reviews with Positive Polarity"""

sam = data.loc[data.polarity == 1,['Review Text']].sample(3).values

for i in sam:
    print(i[0])

"""# Reviews with Neutral Polarity"""

sam = data.loc[data.polarity == 0.5,['Review Text']].sample(3).values

for i in sam:
    print(i[0])

"""# Reviews with Negative Polarity"""

sam = data.loc[data.polarity < 0,['Review Text']].sample(3).values

for i in sam:
    print(i[0])

negative = (len(data.loc[data.polarity <0,['Review Text']].values)/len(data))*100
positive = (len(data.loc[data.polarity >0.5,['Review Text']].values)/len(data))*100
neutral  = len(data.loc[data.polarity >0 ,['Review Text']].values) - len(data.loc[data.polarity >0.5 ,['Review Text']].values)
neutral = neutral/len(data)*100

"""# Pie-Chart about Polarity"""

from matplotlib import pyplot as plt
plt.figure(figsize =(10, 7))
plt.pie([positive,negative,neutral], labels = ['Positive','Negative','Neutral'])

from sklearn.feature_extraction.text import CountVectorizer

def top_n_ngram(corpus,n = None,ngram = 1):
    vec = CountVectorizer(stop_words = 'english',ngram_range=(ngram,ngram)).fit(corpus)
    bag_of_words = vec.transform(corpus) #Have the count of  all the words for each review
    sum_words = bag_of_words.sum(axis =0) #Calculates the count of all the word in the whole review
    words_freq = [(word,sum_words[0,idx]) for word,idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq,key = lambda x:x[1],reverse = True)
    return words_freq[:n]

"""# Visualizing Top 20 Unigrams"""

common_words = top_n_ngram(data['Review Text'], 20,1)
df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])
plt.figure(figsize =(10,5))
df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(
kind='bar', title='Top 20 unigrams in review after removing stop words')

"""# Visualizing Top 20 Bigrams"""

common_words = top_n_ngram(data['Review Text'], 20,2)
df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])
plt.figure(figsize =(10,5))
df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(
kind='bar', title='Top 20 bigrams in review after removing stop words')

"""# Visualizing Top 20 Trigrams"""

common_words = top_n_ngram(data['Review Text'], 20,3)
df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])
plt.figure(figsize =(10,5))
df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(
kind='bar', title='Top 20 trigrams in review after removing stop words')

"""# Visualizing Top 20 Part-of-Speech"""

blob= TextBlob(str(data['Review Text']))
pos = pd.DataFrame(blob.tags,columns =['word','pos'])
pos1 = pos.pos.value_counts()[:20]
plt.figure(figsize = (10,5))
pos1.plot(kind='bar',title ='Top 20 Part-of-speech taggings')

y = data['Recommended IND']

X = data.drop(columns = 'Recommended IND')

"""# Correlation HeatMap"""

import seaborn as sns
sns.heatmap(X.corr(),annot =True)

"""# Handling Multi-Colinearity"""

set1 =set()
cor = X.corr()
for i in cor.columns:
    for j in cor.columns:
        if cor[i][j]>0.8 and i!=j:
            set1.add(i)
print(set1)

X = X.drop(labels = ['token_count'],axis = 1)

X.corr()

class1 =[]
for i in X.polarity:
    if float(i)>=0.0:
        class1.append(1)
    elif float(i)<0.0:
        class1.append(0)
X['sentiment'] = class1

"""# Statistical Description of Data"""

X.groupby(X['sentiment']).describe().T

"""# Model Building"""

import nltk
import re
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

corpus =[]

X.index = np.arange(len(X))

"""# RE + Tokenizing + Stemming + Corpus Creation"""

for i in range(len(X)):
    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])
    review = review.lower()
    review = review.split()
    ps = PorterStemmer()
    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]
    review =' '.join(review)
    corpus.append(review)

"""# Bag of Words Technique

![image.png](attachment:image.png)
"""

from sklearn.feature_extraction.text import CountVectorizer as CV
cv  = CV(max_features = 3000,ngram_range=(1,1))
X_cv = cv.fit_transform(corpus).toarray()
y = y.values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size = 0.20, random_state = 0)

from sklearn.naive_bayes import BernoulliNB
classifier = BernoulliNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, y_pred)

acc

"""# Term Frequency- Inverse Document Frequency Technique

![image.png](attachment:image.png)
"""

from sklearn.feature_extraction.text import TfidfVectorizer as TV
tv  = TV(ngram_range =(1,1),max_features = 3000)
X_tv = tv.fit_transform(corpus).toarray()

X_train, X_test, y_train, y_test = train_test_split(X_tv, y, test_size = 0.20, random_state = 0)
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
acc = accuracy_score(y_test, y_pred)

acc

"""# Deep Learning Model"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = 3000)
tokenizer.fit_on_texts(corpus)

sequences = tokenizer.texts_to_sequences(corpus)
padded = pad_sequences(sequences, padding='post')

word_index = tokenizer.word_index
count = 0
for i,j in word_index.items():
    if count == 11:
        break
    print(i,j)
    count = count+1

embedding_dim = 64
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(3000, embedding_dim),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.summary()

num_epochs = 10

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model.fit(padded,y,epochs= num_epochs)

"""CHECKING NEW EXAMPLE"""

sample_string = "I Will tell my friends for sure"
sample = tokenizer.texts_to_sequences(sample_string)
padded_sample = pad_sequences(sample, padding='post')

padded_sample.T

model.predict(padded_sample.T)